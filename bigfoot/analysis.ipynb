{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Bigfoot Sightings Analysis\n",
        "date: 09-30-2022\n",
        "freeze: auto\n",
        "format: html\n",
        "execute:\n",
        "  enabled: true\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearn import model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this report, I'll be working with Tidy Tuesday [data](https://github.com/rfordatascience/tidytuesday/tree/master/data/2022/2022-09-13) about the important topic of reported Bigfoot sightings. \n",
        "\n",
        "Now, I don't think the evidence supports Bigfoot's existence any more strongly than the Easter Bunny's, but the data should be interesting regardless."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "raw_data = pd.read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-09-13/bigfoot.csv\")\n",
        "raw_data.head() \n",
        "raw_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Bigfoot Field Researchers Organization did their work well: each sighting comes with an impressive amount of metadata. Unfortunately, there are \n",
        "lots of NaN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "raw_data.columns\n",
        "raw_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We may as well convert the date column to the proper data type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "raw_data[\"date\"] = pd.to_datetime(raw_data[\"date\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will predict the region of reported sightings from other variables. \n",
        "\n",
        "# Available Features\n",
        "\n",
        "The \"classification\" column refers to the \n",
        "plausibility of the sighting, with \"C\" denoting \n",
        "second- or third-hand reports. (See [https://www.bfro.net/GDB/classify.asp](https://www.bfro.net/GDB/classify.asp))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y = raw_data[\"state\"]\n",
        "X = raw_data\n",
        "del raw_data[\"state\"]\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = .25, random_state = 12345)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDA \n",
        "\n",
        "Some states have only a few observations (what is Bigfoot doing in Delaware?), which will make prediction more difficult. As we'd expect, most come from the Pacific Northwest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_train.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sightings span a long time range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_kde(X_train.dropna()[\"date\"].dt.year)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Does the distribution of classification types \n",
        "vary by state? Overall, the split is about even between A and B.\n",
        "In some states, class A reports (the most credible) predominate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train[\"classification\"].value_counts()\n",
        "X_train[\"classification\"].groupby([y_train]).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A simple longitude-latitude plot shows a surprising number of sightings on the East Coast. Bigfoot is usually thought of escaping detection in the sparsely settled West.\n",
        "But if we assume all Bigfoot sightings are false or mistaken, we would expect sighting density to correlate with population. That would explain the eastern sightings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# see \n",
        "X_train.plot(x = \"longitude\", y = \"latitude\", kind = \"scatter\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Several of the variables have normal-ish distributions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_density(var): \n",
        "    X_train[var].plot.kde()\n",
        "    plt.title(var)\n",
        "    plt.show()\n",
        "\n",
        "for var in [\"moon_phase\", \"temperature_mid\", \"dew_point\", \"humidity\"]: \n",
        "    plot_density(var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I recode states into regions, rather arbitrarily.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "regions = {\"mid_atlantic\" : [\"Delaware\", \"Maryland\", \"Virginia\", \"West Virginia\", \"New Jersey\" , \"New York\", \"Pennsylvania\"],\n",
        "\"southeast\" : [\"South Carolina\", \"North Carolina\", \"Georgia\", \"Tennessee\", \"Kentucky\", \"Arkansas\", \"Louisiana\", \"Alabama\" , \"Mississippi\", \"Florida\"],\n",
        "\"new_england\" : [\"Rhode Island\", \"Vermont\", \"New Hampshire\", \"Maine\", \"Connecticut\", \"Massachusetts\"], \n",
        "\"midwest\" : [\"Ohio\", \"Illinois\", \"Missouri\", \"Indiana\", \"Oklahoma\", \"Wisconsin\", \"Nebraska\", \"Iowa\", \"Michigan\"], \n",
        "\"central\" : [\"North Dakota\", \"South Dakota\", \"Colorado\", \"Wyoming\", \"Kansas\", \"Texas\", \"Minnesota\", \"Montana\"], \n",
        "\"west\" : [\"Washington\", \"California\", \"Oregon\", \"Utah\", \"New Mexico\", \"Alaska\", \"Nevada\", \"Arizona\", \"Idaho\"]\n",
        "}\n",
        "regions = {**{state: region  for region, v in regions.items() for state in v}}\n",
        "y_train = y_train.map(regions)\n",
        "assert y_train.isnull().sum() == 0\n",
        "\n",
        "classifier = HistGradientBoostingClassifier(learning_rate = .2, min_samples_leaf = 10, l2_regularization = .05,   verbose =0, random_state = 12345)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I do a little feature engineering: \n",
        "\n",
        "I create a `temperature_mean` variable \n",
        "averaging high, mid, and low temperatures Then I \n",
        "use multivariate NaN imputation for the many missing values in the chosen predictors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train[\"temperature_mean\"] = X_train[[ \"temperature_high\", \"temperature_mid\", \"temperature_low\" ]].sum(axis = 1) / 3\n",
        "features =[ \"humidity\", \"cloud_cover\", \"precip_intensity\", \"pressure\", \"visibility\", \"precip_probability\", \"wind_speed\" ]\n",
        "predictors = X_train[features]\n",
        "imputer =  IterativeImputer(max_iter=10, random_state=12345)\n",
        "imputer.fit(predictors)\n",
        "predictors = imputer.transform(predictors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now to fit and evaluate the model. Cross-validated scores aren't very good; the \n",
        "level of accuracy obtained isn't much better than the naive classifier (i.e., \n",
        "predicting the most likely class for each observation).\n",
        "\n",
        "But the accuracy on the training data is about 80%, very good. That suggests overfitting, \n",
        "so I'll try to make the model less flexible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = classifier.fit(predictors, y_train)\n",
        "scores = cross_val_score(classifier, predictors, y_train, cv = 5)\n",
        "y_train.value_counts() /len(y_train)\n",
        "classifier.score(predictors, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try again, this time scaling predictors and reducing flexibility. This improves scores a little."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "scaled = np.apply_along_axis(lambda x: (x - np.mean(x))/ np.var(x), axis = 1, arr = predictors)\n",
        "classifier = HistGradientBoostingClassifier(learning_rate = .03, max_leaf_nodes = 15, n_iter_no_change = 5, verbose = 0, random_state = 12345)\n",
        "model = classifier.fit(predictors, y_train)\n",
        "scores = cross_val_score(classifier, predictors, y_train, cv = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A random forest fails to do better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "forest = RandomForestClassifier(n_estimators = 200,   random_state = 12345,  ccp_alpha = 0.1)\n",
        "model = forest.fit(predictors, y_train)\n",
        "scores = cross_val_score(forest, predictors, y_train, cv = 5)\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediction time. The model again fares badly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_test = y_test.map(regions)\n",
        "X_test[\"temperature_mean\"] = X_test[[ \"temperature_high\", \"temperature_mid\", \"temperature_low\" ]].sum(axis = 1) / 3\n",
        "X_test = X_test[features]\n",
        "X_test = imputer.transform(X_test)\n",
        "X_test= np.apply_along_axis(lambda x: (x - np.mean(x))/ np.var(x), axis = 1, arr = X_test)\n",
        "predictions = classifier.predict(X_test)\n",
        "classifier.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}